<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sample generated videos</title>
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
</head>

<style type=""> .container { background-color: #EBEBEB; } </style>
<body>
    <div class="container">
        <h1 class="text-center my-5">Audio-driven Talking Face Generation by Overcoming Unintended Information Flow</h1>
        <h2 class="text-center my-5">Supplementary Videos</h2>

        <div style="padding-bottom:5%">
        <h3 class="">Ablation Study</h3>
        <p>This video contains output of different setups that were explained under <b>Section 5.3</b> in our paper. The scores were presented in <b>Table 2</b> in the paper.</p>
        <ol>
            <li><FONT COLOR="orange"><b>Setup A:</FONT></b> Base model with original/plain synchronization loss</li>
            <li><FONT COLOR="orange"><b>Setup D:</FONT></b> Base model with silent-lip generation model and pretrained SyncNet audio encoder</li>
            <li><FONT COLOR="orange"><b>Setup E:</FONT></b> Setup D with our proposed stabilized synchronization loss instead of original/plain synchronization loss</li>
            <li><FONT COLOR="orange"><b>Setup G:</FONT></b> Final setup. Setup E with adaptive triplet loss. This setup contains all our contributions.</li>
        </ol>
        </div>

        <div style="padding-bottom:5%">
        <h3 class="">Comparison with the Older Methods</h3>
        <p>Here, we compare our results with <a href>PC-AVS</a>, <a href>Wav2Lip</a> and <a href>EAMM</a> papers. We chose two videos from <a href>HDTF</a> dataset and randomly took a clip from each of them. In the end, we run the published models to generate the videos and combine in a single video by only putting faces to make the comparison easier.</p>

        <h3 class="">Comparison with the Recent Methods</h3>
        <p>Here, we compare our method with the most recent and accurate methods; <a href>DINet (AAAI 2023)</a>, <a href>TalkLip (CVPR 2023)</a>, and <a href>VideoReTalking (SIGGRAPH 2022)</a>. Since the other recent methods in our paper (e.g., SyncTalkFace and LipFormer) have no public models, we were not be able to include them into this comparison.</p>
        </div>

        <h3 class="">Additional Videos Generated by Our Model</h3>
        

    </div>

    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.3/dist/umd/popper.min.js"
            integrity="sha384-0jtkM0gDJxkPQfE+yoK1Rv7I9zj8XV7lQdYUsRvhrNAdxNmmKq3eiqkBM12nVzJa"
            crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
